{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPMI Debugging Notebook\n",
    "This notebook helps identify why the model performance drops to 10% after initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import your modules\n",
    "from dataset import SPMIDataset, get_transforms\n",
    "from model import WideResNet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Dataset cifar10: 100 labeled, 49900 unlabeled instances\n",
      "Average number of candidates per labeled instance: 3.45\n",
      "Dataset: 100 labeled, 49900 unlabeled\n"
     ]
    }
   ],
   "source": [
    "# Create small test dataset\n",
    "dataset = SPMIDataset(\n",
    "    dataset_name='cifar10',\n",
    "    root='./data',\n",
    "    num_labeled=100,  # Small for debugging\n",
    "    partial_rate=0.3,\n",
    "    transform=get_transforms('cifar10', strong_aug=False),\n",
    "    download=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "test_loader = dataset.get_test_loader(batch_size=256)\n",
    "\n",
    "print(f\"Dataset: {len(dataset.labeled_indices)} labeled, {len(dataset.unlabeled_indices)} unlabeled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Standard Supervised Learning\n",
    "First, let's verify that the model can learn with standard cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model_ce\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     11\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, _, targets, _, _ \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     13\u001b[0m     imgs, targets \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m     optimizer_ce\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/reprod/dataset.py:158\u001b[0m, in \u001b[0;36mSPMIDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    155\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m--> 158\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Create candidate label mask\u001b[39;00m\n\u001b[1;32m    161\u001b[0m candidate_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/functional.py:175\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    173\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Test standard supervised learning\n",
    "model_ce = WideResNet(depth=16, widen_factor=2, num_classes=10).to(device)\n",
    "optimizer_ce = optim.SGD(model_ce.parameters(), lr=0.1, momentum=0.9)\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "ce_losses = []\n",
    "ce_accuracies = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    model_ce.train()\n",
    "    total_loss = 0\n",
    "    for imgs, _, targets, _, _ in dataloader:\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer_ce.zero_grad()\n",
    "        outputs = model_ce(imgs)\n",
    "        loss = criterion_ce(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer_ce.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Evaluate\n",
    "    model_ce.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model_ce(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    accuracy = 100.0 * correct / total\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    ce_losses.append(avg_loss)\n",
    "    ce_accuracies.append(accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "print(\"Standard supervised learning works well!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Different SPMI Loss Implementations\n",
    "Now let's test different interpretations of the SPMI loss to see which one works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ce_loss_v1(outputs, candidate_masks):\n",
    "    \"\"\"Version 1: Weighted sum of cross-entropy losses\"\"\"\n",
    "    batch_size = outputs.size(0)\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        candidates = candidate_masks[i].nonzero(as_tuple=True)[0]\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "        \n",
    "        probs = F.softmax(outputs[i], dim=0)\n",
    "        candidate_probs = probs[candidates]\n",
    "        weights = candidate_probs / candidate_probs.sum()\n",
    "        \n",
    "        instance_loss = 0\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            ce_loss = F.cross_entropy(outputs[i:i+1], torch.tensor([candidate], device=outputs.device))\n",
    "            instance_loss += weights[j] * ce_loss\n",
    "        \n",
    "        total_loss += instance_loss\n",
    "    \n",
    "    return total_loss / batch_size\n",
    "\n",
    "\n",
    "def weighted_ce_loss_v2(outputs, candidate_masks):\n",
    "    \"\"\"Version 2: Negative log of weighted sum of probabilities\"\"\"\n",
    "    batch_size = outputs.size(0)\n",
    "    total_loss = 0\n",
    "    \n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        candidates = candidate_masks[i].nonzero(as_tuple=True)[0]\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "        \n",
    "        candidate_probs = probs[i, candidates]\n",
    "        weights = candidate_probs / (candidate_probs.sum() + 1e-10)\n",
    "        \n",
    "        # Negative log of weighted sum of probabilities\n",
    "        weighted_prob_sum = 0\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            weighted_prob_sum += weights[j] * torch.exp(log_probs[i, candidate])\n",
    "        \n",
    "        loss = -torch.log(weighted_prob_sum + 1e-10)\n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_loss / batch_size\n",
    "\n",
    "\n",
    "def kl_loss(outputs, candidate_masks):\n",
    "    \"\"\"Version 3: KL divergence with uniform target\"\"\"\n",
    "    batch_size = outputs.size(0)\n",
    "    num_classes = outputs.size(1)\n",
    "    total_loss = 0\n",
    "    \n",
    "    log_probs = F.log_softmax(outputs, dim=1)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        candidates = candidate_masks[i].nonzero(as_tuple=True)[0]\n",
    "        if len(candidates) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create uniform target distribution over candidates\n",
    "        target_dist = torch.zeros(num_classes).to(outputs.device)\n",
    "        target_dist[candidates] = 1.0 / len(candidates)\n",
    "        \n",
    "        loss = F.kl_div(log_probs[i], target_dist, reduction='sum')\n",
    "        total_loss += loss\n",
    "    \n",
    "    return total_loss / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing V1: Weighted CE\n",
      "Epoch 1: Loss=0.1597, Accuracy=10.01%\n",
      "Epoch 2: Loss=0.1413, Accuracy=10.00%\n",
      "Epoch 3: Loss=0.1358, Accuracy=10.56%\n",
      "Epoch 4: Loss=0.1380, Accuracy=10.21%\n",
      "Epoch 5: Loss=0.1317, Accuracy=10.84%\n",
      "Epoch 6: Loss=0.1299, Accuracy=10.03%\n",
      "Epoch 7: Loss=0.1342, Accuracy=10.00%\n",
      "Epoch 8: Loss=0.1323, Accuracy=10.00%\n",
      "Epoch 9: Loss=0.1375, Accuracy=10.00%\n",
      "Epoch 10: Loss=0.1298, Accuracy=10.63%\n",
      "\n",
      "Testing V2: Neg Log Weighted Prob\n",
      "Epoch 1: Loss=0.1425, Accuracy=10.00%\n",
      "Epoch 2: Loss=0.1281, Accuracy=10.50%\n",
      "Epoch 3: Loss=0.1197, Accuracy=10.00%\n",
      "Epoch 4: Loss=0.1259, Accuracy=10.00%\n",
      "Epoch 5: Loss=0.1321, Accuracy=10.00%\n",
      "Epoch 6: Loss=0.1313, Accuracy=10.36%\n",
      "Epoch 7: Loss=0.1321, Accuracy=10.03%\n",
      "Epoch 8: Loss=0.1267, Accuracy=10.00%\n",
      "Epoch 9: Loss=0.1190, Accuracy=10.00%\n",
      "Epoch 10: Loss=0.1303, Accuracy=10.00%\n",
      "\n",
      "Testing V3: KL Divergence\n",
      "Epoch 1: Loss=0.0926, Accuracy=10.08%\n",
      "Epoch 2: Loss=0.0753, Accuracy=9.93%\n",
      "Epoch 3: Loss=0.0782, Accuracy=9.02%\n",
      "Epoch 4: Loss=0.0743, Accuracy=10.00%\n",
      "Epoch 5: Loss=0.0766, Accuracy=10.00%\n",
      "Epoch 6: Loss=0.0770, Accuracy=10.00%\n",
      "Epoch 7: Loss=0.0767, Accuracy=10.03%\n",
      "Epoch 8: Loss=0.0753, Accuracy=10.00%\n",
      "Epoch 9: Loss=0.0753, Accuracy=10.00%\n",
      "Epoch 10: Loss=0.0732, Accuracy=10.00%\n"
     ]
    }
   ],
   "source": [
    "# Test different loss functions\n",
    "loss_functions = {\n",
    "    'V1: Weighted CE': weighted_ce_loss_v1,\n",
    "    'V2: Neg Log Weighted Prob': weighted_ce_loss_v2,\n",
    "    'V3: KL Divergence': kl_loss\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for loss_name, loss_fn in loss_functions.items():\n",
    "    print(f\"\\nTesting {loss_name}\")\n",
    "    \n",
    "    # Create fresh model\n",
    "    model = WideResNet(depth=16, widen_factor=2, num_classes=10).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for imgs, candidate_masks, targets, _, is_labeled in dataloader:\n",
    "            # Only use labeled data for fair comparison\n",
    "            labeled_mask = (is_labeled == 1)\n",
    "            if not torch.any(labeled_mask):\n",
    "                continue\n",
    "            \n",
    "            imgs = imgs[labeled_mask].to(device)\n",
    "            candidate_masks = candidate_masks[labeled_mask].to(device)\n",
    "            targets = targets[labeled_mask].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, candidate_masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in test_loader:\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        accuracy = 100.0 * correct / total\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        \n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
    "    \n",
    "    results[loss_name] = {\n",
    "        'losses': losses,\n",
    "        'accuracies': accuracies,\n",
    "        'final_accuracy': accuracies[-1]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Analyze Gradient Flow\n",
    "Let's see what happens to gradients with partial label loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing gradients for 1 samples\n",
      "True labels: [0]\n",
      "Candidate masks:\n",
      "  Sample 0: [0 5]\n",
      "\n",
      "Initial predictions:\n",
      "Sample 0: pred=7 (prob=0.123), true=0\n",
      "\n",
      "Standard CE:\n",
      "  Loss value: 2.4846\n",
      "  Avg gradient norm: 0.345685\n",
      "  Max gradient norm: 4.307232\n",
      "\n",
      "Weighted CE V1:\n",
      "  Loss value: 2.2785\n",
      "  Avg gradient norm: 0.245159\n",
      "  Max gradient norm: 2.980722\n",
      "\n",
      "Weighted CE V2:\n",
      "  Loss value: 2.2639\n",
      "  Avg gradient norm: 0.260286\n",
      "  Max gradient norm: 3.176491\n",
      "\n",
      "KL Divergence:\n",
      "  Loss value: 1.6159\n",
      "  Avg gradient norm: 0.232526\n",
      "  Max gradient norm: 2.819656\n"
     ]
    }
   ],
   "source": [
    "# Analyze gradients\n",
    "model = WideResNet(depth=16, widen_factor=2, num_classes=10).to(device)\n",
    "\n",
    "# Get a batch\n",
    "for imgs, candidate_masks, targets, _, is_labeled in dataloader:\n",
    "    labeled_mask = (is_labeled == 1)\n",
    "    if torch.any(labeled_mask):\n",
    "        imgs = imgs[labeled_mask][:4].to(device)\n",
    "        candidate_masks = candidate_masks[labeled_mask][:4].to(device)\n",
    "        targets = targets[labeled_mask][:4].to(device)\n",
    "        break\n",
    "\n",
    "print(f\"Analyzing gradients for {len(imgs)} samples\")\n",
    "print(f\"True labels: {targets.cpu().numpy()}\")\n",
    "print(f\"Candidate masks:\")\n",
    "for i in range(len(imgs)):\n",
    "    candidates = candidate_masks[i].nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "    print(f\"  Sample {i}: {candidates}\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(imgs)\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "print(\"\\nInitial predictions:\")\n",
    "for i in range(len(imgs)):\n",
    "    pred_class = outputs[i].argmax().item()\n",
    "    pred_prob = probs[i].max().item()\n",
    "    print(f\"Sample {i}: pred={pred_class} (prob={pred_prob:.3f}), true={targets[i].item()}\")\n",
    "\n",
    "# Test different losses\n",
    "losses = {\n",
    "    'Standard CE': F.cross_entropy(outputs, targets),\n",
    "    'Weighted CE V1': weighted_ce_loss_v1(outputs, candidate_masks),\n",
    "    'Weighted CE V2': weighted_ce_loss_v2(outputs, candidate_masks),\n",
    "    'KL Divergence': kl_loss(outputs, candidate_masks)\n",
    "}\n",
    "\n",
    "for loss_name, loss in losses.items():\n",
    "    model.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    \n",
    "    # Calculate gradient statistics\n",
    "    grad_norms = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norms.append(param.grad.norm().item())\n",
    "    \n",
    "    print(f\"\\n{loss_name}:\")\n",
    "    print(f\"  Loss value: {loss.item():.4f}\")\n",
    "    print(f\"  Avg gradient norm: {np.mean(grad_norms):.6f}\")\n",
    "    print(f\"  Max gradient norm: {np.max(grad_norms):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Visualize Prediction Evolution\n",
    "Let's see how predictions evolve during training with SPMI loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough labeled samples in batch. Using whatever we have...\n",
      "Tracking 1 samples\n",
      "Step 0: Loss=2.1563\n",
      "Step 10: Loss=0.0000\n",
      "Step 20: Loss=0.0000\n",
      "Step 30: Loss=0.0000\n",
      "Step 40: Loss=0.0000\n",
      "\n",
      "Final predictions:\n",
      "Sample 0: pred=2 (prob=1.000), true=5, candidates=[0 1 2 3 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "model = WideResNet(depth=16, widen_factor=2, num_classes=10).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "# Get test samples - make sure we have enough\n",
    "test_imgs = None\n",
    "test_masks = None\n",
    "test_targets = None\n",
    "\n",
    "for imgs, candidate_masks, targets, _, is_labeled in dataloader:\n",
    "    labeled_mask = (is_labeled == 1)\n",
    "    labeled_indices = torch.where(labeled_mask)[0]\n",
    "    \n",
    "    if len(labeled_indices) >= 4:\n",
    "        # Select exactly 4 labeled samples\n",
    "        selected_indices = labeled_indices[:4]\n",
    "        test_imgs = imgs[selected_indices].to(device)\n",
    "        test_masks = candidate_masks[selected_indices].to(device)\n",
    "        test_targets = targets[selected_indices]\n",
    "        break\n",
    "\n",
    "if test_imgs is None:\n",
    "    print(\"Not enough labeled samples in batch. Using whatever we have...\")\n",
    "    for imgs, candidate_masks, targets, _, is_labeled in dataloader:\n",
    "        labeled_mask = (is_labeled == 1)\n",
    "        if torch.any(labeled_mask):\n",
    "            test_imgs = imgs[labeled_mask].to(device)\n",
    "            test_masks = candidate_masks[labeled_mask].to(device)\n",
    "            test_targets = targets[labeled_mask]\n",
    "            break\n",
    "\n",
    "num_test_samples = len(test_imgs)\n",
    "print(f\"Tracking {num_test_samples} samples\")\n",
    "\n",
    "prediction_history = []\n",
    "loss_history = []\n",
    "\n",
    "# Train and track predictions\n",
    "for step in range(50):\n",
    "    # Record predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_imgs)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        prediction_history.append(probs.cpu().numpy())\n",
    "    \n",
    "    # Training step\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(test_imgs)\n",
    "    \n",
    "    # Use weighted CE v2 (the correct implementation)\n",
    "    loss = weighted_ce_loss_v2(outputs, test_masks)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_history.append(loss.item())\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}: Loss={loss.item():.4f}\")\n",
    "\n",
    "# Plot evolution - adjust for actual number of samples\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(min(4, num_test_samples)):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Extract predictions for this sample\n",
    "    sample_preds = np.array([hist[i] for hist in prediction_history])\n",
    "    \n",
    "    # Plot evolution of each class probability\n",
    "    for class_idx in range(10):\n",
    "        ax.plot(sample_preds[:, class_idx], label=f'Class {class_idx}')\n",
    "    \n",
    "    ax.axhline(y=0.1, color='r', linestyle='--', label='Uniform')\n",
    "    ax.set_title(f'Sample {i} (True: {test_targets[i].item()})')\n",
    "    ax.set_xlabel('Training Step')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Mark candidate labels\n",
    "    candidates = test_masks[i].nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "    ax.set_title(f'Sample {i} (True: {test_targets[i].item()}, Candidates: {candidates})')\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Hide unused subplots\n",
    "for i in range(num_test_samples, 4):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('prediction_evolution.png', bbox_inches='tight', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Plot loss evolution\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Evolution')\n",
    "plt.grid(True)\n",
    "plt.savefig('loss_evolution.png', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Print final predictions\n",
    "print(\"\\nFinal predictions:\")\n",
    "final_probs = prediction_history[-1]\n",
    "for i in range(num_test_samples):\n",
    "    max_prob = final_probs[i].max()\n",
    "    pred_class = final_probs[i].argmax()\n",
    "    candidates = test_masks[i].nonzero(as_tuple=True)[0].cpu().numpy()\n",
    "    print(f\"Sample {i}: pred={pred_class} (prob={max_prob:.3f}), true={test_targets[i].item()}, candidates={candidates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: Why Does It Converge to Uniform?\n",
    "Let's analyze why the weighted loss causes uniform predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing different prediction scenarios:\n",
      "Candidate labels: [0, 1, 2]\n",
      "\n",
      "Peaked predictions:\n",
      "  Outputs: [5.  0.1 0.1]\n",
      "  Probabilities: [0.93719035 0.00697887 0.00697887]\n",
      "  Candidate probs: [0.93719035 0.00697887 0.00697887]\n",
      "  Weights: [0.9853254  0.00733731 0.00733731]\n",
      "  V1 Loss: 0.1368\n",
      "  V2 Loss: 0.0795\n",
      "\n",
      "Semi-uniform predictions:\n",
      "  Outputs: [1. 1. 1.]\n",
      "  Probabilities: [0.17105748 0.17105748 0.17105748]\n",
      "  Candidate probs: [0.17105748 0.17105748 0.17105748]\n",
      "  Weights: [0.3333333 0.3333333 0.3333333]\n",
      "  V1 Loss: 1.7658\n",
      "  V2 Loss: 1.7658\n",
      "\n",
      "Uniform predictions:\n",
      "  Outputs: [0. 0. 0.]\n",
      "  Probabilities: [0.1 0.1 0.1]\n",
      "  Candidate probs: [0.1 0.1 0.1]\n",
      "  Weights: [0.3333333 0.3333333 0.3333333]\n",
      "  V1 Loss: 2.3026\n",
      "  V2 Loss: 2.3026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze the feedback loop\n",
    "def analyze_weighted_loss_behavior():\n",
    "    \"\"\"Analyze why weighted loss leads to uniform predictions\"\"\"\n",
    "    \n",
    "    # Create synthetic example\n",
    "    batch_size = 1\n",
    "    num_classes = 10\n",
    "    \n",
    "    # Simulate different prediction scenarios\n",
    "    scenarios = {\n",
    "        'Peaked': torch.tensor([[5.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]),\n",
    "        'Semi-uniform': torch.tensor([[1.0, 1.0, 1.0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]]),\n",
    "        'Uniform': torch.zeros(1, 10)\n",
    "    }\n",
    "    \n",
    "    # Create candidate mask (3 candidates)\n",
    "    candidate_mask = torch.zeros(1, 10)\n",
    "    candidate_mask[0, [0, 1, 2]] = 1\n",
    "    \n",
    "    print(\"Analyzing different prediction scenarios:\")\n",
    "    print(\"Candidate labels: [0, 1, 2]\")\n",
    "    print()\n",
    "    \n",
    "    for scenario_name, outputs in scenarios.items():\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        log_probs = F.log_softmax(outputs, dim=1)\n",
    "        \n",
    "        # Get candidate probabilities\n",
    "        candidates = candidate_mask[0].nonzero(as_tuple=True)[0]\n",
    "        candidate_probs = probs[0, candidates]\n",
    "        \n",
    "        # Calculate weights\n",
    "        weights = candidate_probs / candidate_probs.sum()\n",
    "        \n",
    "        # Calculate losses\n",
    "        v1_loss = 0\n",
    "        v2_loss = 0\n",
    "        \n",
    "        # V1: Weighted sum of CE losses\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            ce_loss = F.cross_entropy(outputs, torch.tensor([candidate]))\n",
    "            v1_loss += weights[j] * ce_loss\n",
    "        \n",
    "        # V2: Negative log of weighted sum of probabilities\n",
    "        weighted_prob_sum = 0\n",
    "        for j, candidate in enumerate(candidates):\n",
    "            weighted_prob_sum += weights[j] * probs[0, candidate]\n",
    "        v2_loss = -torch.log(weighted_prob_sum)\n",
    "        \n",
    "        print(f\"{scenario_name} predictions:\")\n",
    "        print(f\"  Outputs: {outputs[0][:3].numpy()}\")\n",
    "        print(f\"  Probabilities: {probs[0][:3].numpy()}\")\n",
    "        print(f\"  Candidate probs: {candidate_probs.numpy()}\")\n",
    "        print(f\"  Weights: {weights.numpy()}\")\n",
    "        print(f\"  V1 Loss: {v1_loss.item():.4f}\")\n",
    "        print(f\"  V2 Loss: {v2_loss.item():.4f}\")\n",
    "        print()\n",
    "\n",
    "analyze_weighted_loss_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The issue is that the weighted cross-entropy loss (V1) creates a feedback loop:\n",
    "1. When predictions become uniform, all candidates get equal weight\n",
    "2. This encourages the model to predict all candidates equally\n",
    "3. The model converges to uniform predictions (10% accuracy)\n",
    "\n",
    "The correct implementation (V2) calculates the negative log of the weighted sum of probabilities, which doesn't have this issue.\n",
    "\n",
    "To fix your implementation, use the V2 loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
